# Bact-Builder Pipeline Tutorial: Execution
## Description:
The purpose of this tutorial is to demonstrate the overall workflow for Bact-Builder. Bact-Builder was designed to provide an end to end solution for assembling complete bactertial genomes. It can take raw fast5 files generated by Oxford Nanopore sequencers (ex. MinION), basecall, filter, assemble and polish bacterial genomes. It takes advantage of multiple assemblers that are then combined into a consensus genome that is further polished to generate highly accurate, closed out bacterial genomes.  
*This tutorial will detail the instructions to execute the pipeline. Extended workflow examples can be found [here]()*

## System Requirements:
Bact-Builder utilizes the [Nextflow](https://www.nextflow.io/) workflow manager and requires a POSIX environment, such as the various flavors of linux or macOS. Through the use of Nextflow, Bact-Builder can be run flexibly on a local machine, available high performance compute cluster, or on one of the various cloud providers. Due to the large demands of the various software tools, we **strongly recommend** utilizing an HPC environment with a minimum of 30 CPU cores and 180Gb of RAM per node, as well as Singularity (v3.5.2). The base configuration is designed for a slurm controller, but can be customized to fit your personal system as well. 

To install Nextflow you can follow the directions found [on their main page](https://www.nextflow.io/). In brief, ensure you have Java v8+ and then download the executable:
```
curl -s https://get.nextflow.io | bash
```
The nextflow executable can then be found in your current working directory, and can either be used as such or added to your user's PATH depending upon your needs. 
### Containerized execution
Bact-Builder is designed to optimally work with containerized environments. Locally this is often [Docker](https://www.docker.com/), and in an HPC setting is [Singularity](https://sylabs.io/singularity/). To install Docker, please follow the [appropriate installation steps for your operating system.](https://docs.docker.com/get-docker/). To install Singularity, please contact your local HPC administrator to appropriately ensure the installation. 

When executing the pipeline, you can utilize the ```-profile docker``` or ```-profile singularity``` to automatically pull the appropriate containers for each step. 

### Full local execution
Full local execution is not recommended. If you are to do this, you must first install each individual tool per the developer's recommendations and ensure it is included in your PATH. Good luck...

## Metadata
Successful execution of this pipeline requires an accurate metadata table. This is a comma-delimited file (.csv) that contains the appropriate paths and sample names for the pipeline ingress. Examples can be found at [within the documentation](https://github.com/alemenze/bact-builder/tree/main/docs/metadata_examples). For this example, we will demonstrate making a table purely via command line, and will be demonstrating a full pipeline execution for a singular sample. If you are using a subset workflow, please ensure the metadata table contains the appropriate columns. 

```bash
touch metadata_example.csv
echo "sample_id,fast5_dir,fast5_dirname,ont_barcode,illumina_r1,illumina_r2" >> metadata_example.csv
echo "ExampleSample,/projectdir/MyFolder/ONT_Data/Run1/,Run1,barcode05,/projectdir/MyFolder/IlluminaData/ExampleSample_R1.fastq.gz,/projectdir/MyFolder/IlluminaData/ExampleSample_R2.fastq.gz" >> metadata_example.csv
```

## Pipeline Execution
Pipeline execution is most often run locally via Docker, through a slurm executor on an HPC (multi-node capacity), or via cloud resources. We will demonstrate each below assuming your data and nextflow are in your home directory. *Locally installed tools and single-node HPC execution are strongly discouraged and will not be demonstrated*

### Locally via Docker
```
~/nextflow run alemenze/bact-builder -r main --samplesheet ~/metadata_example.csv
```

### Slurm executor on an HPC
In this case, we also will set it to run in the background. This is so we can easily log out of the HPC and ensure it continues running. 
```
module load java/11.0.5
module load singularity/3.5.2

nohup ~/nextflow -bg run alemenze/bact-builder --samplesheet ~/metadata_example.csv -profile slurm --node_partition='node_partition' > output_log.txt
```

Additionally, if you have GPU access for guppy basecalling it is strongly recommended to utilize this. You must first ensure you are using Nvidia hardware with CUDA installed. 
```
module load java/11.0.5
module load singularity/3.5.2

nohup ~/nextflow -bg run alemenze/bact-builder --samplesheet ~/metadata_example.csv -profile slurm --node_partition='node_partition' --gpu_active --gpus 1 --gpu_node_partition='gpu_partition' > output_log.txt
```

### Cloud resources
Currently we have tested this using the [Google Cloud Platform](https://cloud.google.com/), though this should be translatable to other cloud service providers as well. 

```
~/nextflow run alemenze/bact-builder --samplesheet ~/metadata_example.csv -profile google --google_bucket gs://bact-builder/output 
```
**This will require you to have your Google cloud account set up for API access and a local json key in your environment**
```
export GOOGLE_APPLICATION_CREDENTIALS=~/key.json
```

## In-house Workflow notes
One thing to consider is your current infrastructure. In our case, we split out the workflow into four steps due to HPC constraints:  
1. Demultiplexing- We do our initial processing as a subworkflow with the ```--only_demux``` parameter and additional controls over the GPU selection. The [Amarel HPC at Rutgers](https://oarc.rutgers.edu/resources/amarel/) is split across the campuses, with the New Brunswick instances currently containing the GPU based nodes. By adding the commands ```'--gpu_clusterOptions=--gres=gpu:1 --nodelist=pascal007'``` we specifically submit the job to nodes we have non-preemptible access to. Example full command: 
```
nohup ~/nextflow -bg run alemenze/bact-builder -r main --samplesheet metadata_example.csv -profile slurm --node_partition='smallnodes_partition' --gpu_active --gpus 1 '--gpu_clusterOptions=--gres=gpu:1 --nodelist=specialNode001' --only_demux --outdir ./example_demux > example_demux_log.txt
```
2. Assembly steps- Our primary node collection are found on the Newark instances of Amarel. After we have processed several flowcells through the above step, we collate the metadata to submit an assembly queue. Example full command:
```
nohup ~/nextflow -bg run alemenze/bact-builder -r main --samplesheet ./metadata_example.csv -profile slurm --node_partition='largenodes_partition' --skip_demux --outdir ./example_assemblies > example_assemblies_log.txt
```
3. Manual Trycycler steps- As much as we have tried to automate this step, this is still under active development to clean up. The first step in the Trycycler workflow often requires removal of some assemblies, which mandates the manual intervention. Future versions will aim to have this manual intervention removed. Example commands:
```
fastas=($(find ./ -type f -name "*.fasta" -print0))
echo ${fastas}
fastas2=$(echo ${fastas} | sed "s/\.fasta\./\.fasta\ \./g")
echo ${fastas2}

# Initial trycycler clustering
docker run -v ~/input_sample_assembly_dir/:/data/ -w /data/ alemenze/trycycler-docker trycycler cluster --assemblies ${fastas2} --reads ~/input_demux_file.fastq.gz --min_contig_depth 0.5 --out_dir ./trycycler

# Trycycler reconciliation step. This is where the manual intervention takes place. After executing this command, you would need to go into the cluster_001 directory and remove offending assemblies. In-house, we would only accept a sample that retained at least 6 total assemblies across at least 2 unique assemblers. 
docker run -v ~/input_sample_assembly_dir/:/data/ -w /data/ alemenze/trycycler-docker trycycler reconcile --reads input_demux_file.fastq.gz --cluster_dir ./trycycler/cluster_001/ --max_length_diff 1.3 --min_identity 95 --max_add_seq 10000 --max_indel_size 1000

# Trycycler multiple sequence alignment step
docker run -v ~/input_sample_assembly_dir/:/data/ -w /data/ alemenze/trycycler-docker trycycler msa --cluster_dir ./trycycler/cluster_001/

# Trycycler partitioning step
docker run -v ~/input_sample_assembly_dir/:/data/ -w /data/ alemenze/trycycler-docker trycycler partition --reads ~/input_demux_file.fastq.gz --cluster_dir ./trycycler/cluster_001/

# Trycycler consensus step
docker run -v ~/input_sample_assembly_dir/:/data/ -w /data/ alemenze/trycycler-docker trycycler consensus --cluster_dir ./trycycler/cluster_001/

# And your final output would be found at ~/trycycler/cluster_001/7_final_consensus.fasta 
# Generally you would rename it to the appropriate sample ID as is done in the pipeline here. 
```
4. Polishing steps- We often have to wait for the Illumina polishing sequences to be completed. In that time, we collect Trycycler assemblies for this final step, collate the metadata with the Illumina information, and submit a large batch on our primary node collection. In a perfect world the Illumina data would be ready from the onset of ONT demux, but logistics often get in the way of that. Example full command:
```
nohup ~/nextflow -bg run alemenze/bact-builder -r main --samplesheet ./metadata_example.csv -profile slurm --node_partition='largenodes_partition' --only_polish --outdir ./example_polishing > example_polish_log.txt
```